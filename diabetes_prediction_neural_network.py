# -*- coding: utf-8 -*-
"""Diabetes_Prediction_Neural_Network

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Nl0X0okm1azCS5W-LDUthbybObyA6Dn

**Binary Classification: Detect diabetes using Neural Network**
"""

### importing libraries

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC
from keras import regularizers
import random
import os

### random seed for reproducibility

seed_value = 6
tf.random.set_seed(seed_value)
os.environ['PYTHONHASHSEED'] = str(seed_value)
np.random.seed(seed_value)
random.seed(seed_value)

# Regulizer coefficient
lamb = 0.001

# flag for L2/Dropout regularization selection(L2 = 0, Dropout = 1)
flag = 1

### importing a shuffled csv file

diabetes = pd.read_csv(r'/content/diabetes.csv').sample(frac=1).reset_index(drop=True)

# Make a copy of dataframe
df = diabetes.copy()

# Display top 3 rows and size of dataset
display(df.head(3))
print('\nSize of the dataset: ', len(df))

# scaling data
df = (df-df.mean(axis='index'))/(df.max()-df.min())
df.head(3)

# resetting actual outcome to original values after scaling: See the "outcome" Column changed back to original values [0, 1].
df['Outcome'] = diabetes['Outcome'].copy()
df.head(3)

### Splitting the dataset into train/validate/test at 60:20:20 ratio

train_df, temp_df = train_test_split( df, test_size=0.4, random_state=seed_value)
val_df, test_df = train_test_split( temp_df, test_size=0.5, random_state=seed_value)

### Separate the dataset into features variable (x_) and target variable (y_)

y_train = train_df['Outcome'].copy()
x_train = train_df.iloc[:,:8]

y_val = val_df['Outcome'].copy()
x_val = val_df.iloc[:,:8]

y_test = test_df['Outcome'].copy()
x_test = test_df.iloc[:,:8]

"""### Neural Network Model Initialization"""

### Neural Network Model initialization using L2 regulizer.

if flag == 0:
    model = keras.Sequential([
        keras.layers.Dense(100, kernel_regularizer=regularizers.l2(lamb), activation=tf.nn.relu),
        keras.layers.Dense(500, kernel_regularizer=regularizers.l2(lamb), activation=tf.nn.relu),
        keras.layers.Dense(500, kernel_regularizer=regularizers.l2(lamb), activation=tf.nn.relu),
        keras.layers.Dense(1, kernel_regularizer=regularizers.l2(lamb), activation=tf.nn.sigmoid),
    ])

### Neural Network Model initialization using Dropout regulizer.

if flag == 1:
  model = keras.Sequential([
          keras.layers.Dropout(0.2, input_shape=(8,)),
          keras.layers.Dense(100, activation=tf.nn.relu),
          keras.layers.Dense(500, activation=tf.nn.relu),
          keras.layers.Dense(500, activation=tf.nn.relu),
          keras.layers.Dense(1, activation=tf.nn.sigmoid),
      ])

"""### Training of the model"""

### Training of the model

model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=200, batch_size=20, validation_data=(x_val, y_val), verbose=0)

### Printing Loss and Accuracy of the model for training and validation data

print('Train_loss: ', history.history['loss'][-1]*100)
print('Train_accuracy: ', history.history['accuracy'][-1]*100)
print('\nVal_loss: ', history.history['val_loss'][-1]*100)
print('Val_accuracy: ', history.history['val_accuracy'][-1]*100)

### Model performance on test dataset

test_res = model.evaluate(x_test, y_test, verbose=0)
print('\nTest_loss: ', test_res[0]*100)
print('Test_accuracy: ', test_res[1]*100)



### Plot of Accuracy: Training vs Validation data

plt.figure(figsize=(6, 4))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy: Training vs Validation Data', fontsize=16)
plt.ylabel('Accuracy', fontsize=12)
plt.xlabel('Epoch', fontsize=12)
plt.legend(['Train', 'Val'], loc='lower right')
plt.grid()
plt.show()

### Plot of Loss: Training vs Validation data

plt.figure(figsize=(6, 4))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss: Training vs Validation Data', fontsize=16)
plt.ylabel('Loss', fontsize=12)
plt.xlabel('Epoch', fontsize=12)
plt.legend(['Train', 'Val'], loc='upper right')
plt.grid()
plt.show()

"""Confusion Matrix Plot"""

# Predictions
predictions = (model.predict(x_test, verbose=0) > 0.5).astype("int32")

# Compute confusion matrix
conf_mat = confusion_matrix(y_test, predictions)

# Create a figure and axes
fig, ax = plt.subplots(figsize=(5,5))

# Ignore warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=info, 2=warning, 3=error
tf.get_logger().setLevel('ERROR')

# Plot the grid
for i in range(2):
    for j in range(2):
        # Green for correct (TP/TN), Red for wrong (FP/FN)
        if i == j:
            color = 'green'
        else:
            color = 'red'
        ax.add_patch(plt.Rectangle((j, i), 1, 1, color=color, alpha=0.5))

        # Add text (confusion matrix value)
        ax.text(j + 0.5, i + 0.5, conf_mat[i,j],
                ha='center', va='center', fontsize=16, color='black')

# Set labels
ax.set_xticks([0.5, 1.5])
ax.set_xticklabels(['Prediction 0', 'Prediction 1'])
ax.set_yticks([0.5, 1.5])
ax.set_yticklabels(['True 0', 'True 1'])

# Set limits and grid
ax.set_xlim(0, 2)
ax.set_ylim(0, 2)
ax.set_aspect('equal')
ax.grid(False)

plt.title('Confusion Matrix (Green=Correct, Red=Wrong)')
plt.show()

# ### Plot of Confusion matrix

# predictions = (model.predict(x_test, verbose=0) > 0.5).astype("int32")
# clf = SVC(random_state=0)
# clf.fit(x_test, y_test)
# conf_mat = (confusion_matrix(y_test,predictions))
# disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=clf.classes_)
# disp.plot()
# plt.title('Confusion matrix')
# plt.show()

"""## ------------------------------------------- END ----------------------------------------------"""